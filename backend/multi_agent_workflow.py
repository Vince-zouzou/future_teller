from langchain_community.chat_models.openai import ChatOpenAI
from typing import List, Dict, Any, Optional, Tuple
import json
import requests
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain.tools import BaseTool
import networkx as nx
import re
import streamlit as st

# Tavily search tool configuration
TAVILY_API_KEY = st.secrets["tavily_key"]
TAVILY_API_URL = "https://api.tavily.com/search"

class TavilySearchTool(BaseTool):
    name: str = "tavily_search"
    description: str = "Use Tavily search engine to search for related information"
    
    def _run(self, query: str) -> List[Dict[str, Any]]:
        """Execute Tavily search"""
        #st.write(query)
        headers = {
            "Authorization": f"Bearer {TAVILY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "query": query,
            "max_results": 10,
            "include_answer": False,
            "include_images": False,
            "topic": "news"
        }

        try:
            response = requests.post(TAVILY_API_URL, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            # Format search results
            formatted_results = []
            for result in data.get('results', []):
                formatted_results.append({
                    "title": result.get('title', ''),
                    "url": result.get('url', ''),
                    "content": result.get('content', ''),
                    "published_date": result.get('published_date', ''),
                    "score": result.get('score', 0)
                })
            
            return formatted_results
            
        except Exception as e:
            return [{"error": f"Search failed: {str(e)}"}]

class KnowledgeGraphTool(BaseTool):
    name: str = "knowledge_graph_query"
    description: str = "Query knowledge graph information"
    
    def _run(self, entities_str: str) -> Dict[str, Any]:
        """Query knowledge graph (using simple in-memory knowledge graph example)"""
        
        # Convert string to entity list
        try:
            entities = json.loads(entities_str)
        except:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„åˆ†å‰²
            entities = entities_str.split(',')
        
        # This can be replaced with actual knowledge graph query logic
        # Example: Return related relationships based on entity names
        knowledge_base = {
            "Trump": {
                "type": "Person",
                "attributes": ["Former US President", "Businessman", "Politician"],
                "relationships": {
                    "Tariff War": {"type": "Initiated", "weight": 0.9},
                    "Musk": {"type": "Knows", "weight": 0.7}
                }
            },
            "Musk": {
                "type": "Person",
                "attributes": ["Tesla CEO", "SpaceX Founder", "Entrepreneur"],
                "relationships": {
                    "Tesla": {"type": "Founded", "weight": 1.0},
                    "Tariff War": {"type": "Affected by", "weight": 0.8}
                }
            }
        }
        
        result = {}
        for entity in entities:
            if entity.strip() in knowledge_base:
                result[entity.strip()] = knowledge_base[entity.strip()]
        
        return result

class ProbabilityCalculatorTool(BaseTool):
    name: str = "probability_calculation"
    description: str = "è®¡ç®—äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡"
    
    def _run(self, input_data: str) -> Dict[str, float]:
        """åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯è®¡ç®—äº‹ä»¶æ¦‚ç‡"""
        
        # è§£æè¾“å…¥æ•°æ®
        try:
            data = json.loads(input_data)
            event_description = data.get("event_description", "")
            context = data.get("context", {})
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²ä½œä¸ºäº‹ä»¶æè¿°
            event_description = input_data
            context = {}
        
        # è¿™é‡Œä½¿ç”¨åŸºäºè§„åˆ™çš„æ¦‚ç‡ä¼°ç®—
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹
        
        probabilities = {}
        
        # Example probability calculation logic
        if "tariff war" in event_description and "Trump" in event_description:
            probabilities["High probability of occurrence"] = 0.7
            probabilities["Medium probability of occurrence"] = 0.2
            probabilities["Low probability of occurrence"] = 0.1

        elif "bankruptcy" in event_description and "Musk" in event_description:
            probabilities["High probability of occurrence"] = 0.3
            probabilities["Medium probability of occurrence"] = 0.4
            probabilities["Low probability of occurrence"] = 0.3

        else:
            # Default probability distribution
            probabilities["Occurrence"] = 0.5
            probabilities["Non-occurrence"] = 0.5
            
        return probabilities

class KnowledgeGraphConverter:
    """Knowledge Graph Converter - Uses AI to convert text information into knowledge graph triplets"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def text_to_triplets(self, text: str, query: str = "") -> List[Tuple[str, str, str]]:
        """Use AI to convert text to knowledge graph triplets"""
        prompt = f"""
        Based on the following text content and query context, extract knowledge graph triplets (subject-relation-object):
        
        Query context: {query}
        Text content: {text[:4000]}  # Limit text length
        
        Extract important entity relationship triplets, focusing on:
        1. Relationships between people, organizations, events
        2. Causal relationships, influence relationships, cooperation relationships
        3. Temporal sequences and logical relationships
        4. Specific behaviors and actions
        
        Return format must be strict Python list format:
        [("subject", "relation", "object"), ("subject", "relation", "object"), ...]
        
        Each triplet should contain:
        - Subject: specific entity or concept
        - Relation: clear relationship description (e.g., influences, causes, cooperates with, belongs to)
        - Object: specific entity or concept
        
        Example:
        [("Trump", "initiated", "tariff war"), ("tariff war", "affected", "global economy"), ("Musk", "was affected by", "tariff war")]
        
        Ensure the extracted relationships are accurate, specific, and meaningful in English.
        """
        
        try:
            response = self.llm.invoke(prompt)
            
            # ä»å“åº”ä¸­æå–ä¸‰å…ƒç»„
            import ast
            triplets = ast.literal_eval(response.content)
            
            if isinstance(triplets, list) and all(len(item) == 3 for item in triplets):
                return triplets
            else:
                # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç©ºåˆ—è¡¨
                return []
                
        except Exception as e:
            print(f"AIçŸ¥è¯†å›¾è°±è½¬æ¢å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨çš„è§„åˆ™åŒ¹é…
            return self._fallback_text_to_triplets(text)
    
    def _fallback_text_to_triplets(self, text: str) -> List[Tuple[str, str, str]]:
        """å¤‡ç”¨çš„è§„åˆ™åŒ¹é…æ–¹æ³•"""
        triplets = []
        
        # ç®€å•çš„è§„åˆ™åŒ¹é…
        patterns = [
            # äººç‰©-åŠ¨ä½œ-å¯¹è±¡æ¨¡å¼
            (r'(\w+)(?:çš„|)(?:å…³ç¨æˆ˜|æ”¿ç­–|å†³å®š)(?:å¯¹|å½±å“)(\w+)', 'å½±å“', 'å¯¹è±¡'),
            (r'(\w+)(?:å¯èƒ½|å°†ä¼š)(å¯¼è‡´|é€ æˆ)(\w+)', 'å¯¼è‡´', 'ç»“æœ'),
            (r'(\w+)(?:å’Œ|ä¸)(\w+)(?:çš„|)(å…³ç³»|åˆä½œ)', 'å…³ç³»', 'å¯¹è±¡'),
            (r'(\w+)(?:å±äº|æ˜¯)(\w+)', 'å±äº', 'ç±»åˆ«'),
            (r'(\w+)(?:åœ¨|äº)(\w+)(?:å‘ç”Ÿ)', 'å‘ç”Ÿåœ¨', 'åœ°ç‚¹'),
        ]
        
        for pattern, relation, _ in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if len(match.groups()) >= 2:
                    subject = match.group(1)
                    obj = match.group(2)
                    triplets.append((subject, relation, obj))
        
        return triplets
    
    @staticmethod
    def create_knowledge_graph(triplets: List[Tuple[str, str, str]]) -> nx.DiGraph:
        """åˆ›å»ºçŸ¥è¯†å›¾è°±ç½‘ç»œ"""
        G = nx.DiGraph()
        
        for subject, relation, obj in triplets:
            G.add_node(subject, type="entity")
            G.add_node(obj, type="entity")
            G.add_edge(subject, obj, relation=relation, label=relation)
        
        return G

class SupervisorAgent:
    """Supervisor Agent - Determines user intent and routes to appropriate workflow"""
    
    def __init__(self, llm):
        self.llm = llm
        
    def analyze_intent(self, query: str) -> str:
        """Analyze user intent"""
        prompt = f"""
        Analyze the intent of the following user query and return the corresponding processing type:
        
        Available types:
        - workflow1: Asking about potential future impacts of events and the likelihood of events occurring  (e.g., What impact will Trump's tariff war have on the world landscape)
        - normal: Casual chat or simple Q&A (e.g., hello hows your day)
        
        User query: {query}
        
        Return only one of: workflow1 or normal.
        """
        
        response = self.llm.invoke(prompt)
        intent = response.content.strip().lower()
        
        if intent in ['workflow1', 'workflow2', 'normal']:
            return intent
        else:
            # é»˜è®¤å¤„ç†å¤æ‚æŸ¥è¯¢
            if any(keyword in query.lower() for keyword in ['å½±å“', 'ç»“æœ', 'å°†ä¼š', 'å¯èƒ½']):
                return 'workflow1'
            elif any(keyword in query.lower() for keyword in ['æ˜¯å¦', 'ä¼šä¸ä¼š', 'å¯èƒ½æ€§', 'æ¦‚ç‡']):
                return 'workflow2'
            else:
                return 'normal'

class Workflow2Agent:
    """Workflow 1 - Analyzes potential future impacts of events"""
    
    def __init__(self, llm, prob_tool, kg_tool):
        self.llm = llm
        self.prob_tool = prob_tool
        self.kg_tool = kg_tool
    
    def process(self, query: str, status_callback=None) -> Dict[str, Any]:
        """å¤„ç†workflow2æŸ¥è¯¢"""
        # æå–å…³é”®å®ä½“
        if status_callback:
            status_callback("Extracting key entities from query")
        entities = self._extract_entities(query)
        
        # æŸ¥è¯¢çŸ¥è¯†å›¾è°± - ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‚æ•°ç±»å‹
        if status_callback:
            status_callback("Querying knowledge graph for entity relationships")
        entities_str = json.dumps(entities)
        kg_info = self.kg_tool.run(entities_str)
        
        # è®¡ç®—æ¦‚ç‡ - ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‚æ•°ç±»å‹
        if status_callback:
            status_callback("Calculating event probabilities based on context")
        prob_input = json.dumps({
            "event_description": query,
            "context": kg_info
        })
        probabilities = self.prob_tool.run(prob_input)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        if status_callback:
            status_callback("Generating comprehensive probability analysis report")
        report = self._generate_report(query, probabilities, kg_info)
        
        return {
            "probabilities": probabilities,
            "knowledge_graph": kg_info,
            "analysis_report": report
        }

    def _extract_entities(self, query: str) -> List[str]:
        """Extract entities from query"""
        prompt = f"""
        Extract key entity names from the following query:
        Query: {query}
        
        Return entity names in JSON array format, e.g.: ["entity1", "entity2", "entity3"]
        """
        
        response = self.llm.invoke(prompt)
        return json.loads(response.content)

    def _generate_report(self, query: str, probabilities: Dict[str, float], kg_info: Dict) -> str:
        """Generate probability analysis report"""
        prompt = f"""
        Based on the following information, generate a detailed analysis report:
        
        Original query: {query}
        Probability analysis results: {json.dumps(probabilities, ensure_ascii=False)}
        Knowledge graph information: {json.dumps(kg_info, ensure_ascii=False)}
        
        Please provide a detailed probability analysis report in the following format:

        ### Short-term (next 1-6 months)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        ### Medium-term (6 months - 2 years)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        ### Long-term (2+ years)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        Requirements:
        1. Provide 5-8 main scenarios for each time period (minimum 5, maximum 8)
        2. Total probability for each time period should be 100%
        3. Clearly cite knowledge graph triplets as basis
        4. Scenario descriptions should be specific, detailed, and have practical basis
        5. Respond in English
        """
        
        response = self.llm.invoke(prompt)
        return response.content

class Workflow1Agent:
    """Workflow 2 - Calculates the likelihood of event occurrence"""
    
    def __init__(self, llm, search_tool, kg_tool, prob_tool):
        self.llm = llm
        self.search_tool = search_tool
        self.kg_tool = kg_tool
        self.prob_tool = prob_tool
        self.converter = KnowledgeGraphConverter(llm)
    
    def process(self, query: str, status_callback=None) -> Dict[str, Any]:
        back = st.session_state.back
        """å¤„ç†workflow1æŸ¥è¯¢ - æŒ‰ç…§æ—¶é—´çº¿æ¢³ç† â†’ è¡¥å……æœç´¢ â†’ æ„å»ºä¸‰å…ƒç»„ â†’ åˆ†æçš„æµç¨‹"""
        # Step 1: åˆæ­¥æœç´¢å»ºç«‹æ—¶é—´çº¿æ¡†æ¶
        if status_callback:
            status_callback("ğŸ” Start initial research")
        initial_keywords = self._generate_timeline_search_terms(query)
        initial_results = []
        for keyword in initial_keywords:
            results = self.search_tool.run(keyword)
            initial_results.extend(results)
        back.write("Initial seach result:")
        back.write(initial_results)
        

        # Step 2: åˆ›å»ºåˆæ­¥æ—¶é—´çº¿
        if status_callback:
            status_callback("ğŸ“… Create timeline")
        timeline = self._create_detailed_timeline(initial_results)
        back.write("timeline:")
        back.write(timeline)
            
        # Step 3: åŸºäºæ—¶é—´çº¿ä¿¡æ¯è¡¥å……æœç´¢
        if status_callback:
            status_callback("ğŸ” Find More information based on current information")
        supplemental_keywords = self._generate_supplemental_search_terms(query, timeline)
        supplemental_results = []
        for keyword in supplemental_keywords:
            results = self.search_tool.run(keyword)

            supplemental_results.extend(results)
        back.write("supplemental_results:")
        back.write(supplemental_results)


        # Step 4: æ›´æ–°å’Œå®Œå–„æ—¶é—´çº¿
        if status_callback:
            status_callback("ğŸ“Š Update timeline and information")
        updated_timeline = self._update_timeline_with_supplemental(timeline, supplemental_results)
        back.write("updated_timeline:")
        back.write(updated_timeline)

        # Step 5: ä»æ‰€æœ‰ä¿¡æ¯ä¸­æå–ä¸‰å…ƒç»„
        if status_callback:
            status_callback("ğŸ”— Extract Relationship")
        all_results = initial_results + supplemental_results
        triplets = self._extract_triplets_from_all_sources(all_results, query)
        back.write("triplets:")
        back.write(triplets)

        # Step 6: æ„å»ºçŸ¥è¯†å›¾è°±
        if status_callback:
            status_callback("ğŸ•¸ï¸ Constract Knowledge graph")
        kg_data = self._create_knowledge_graph_from_triplets(triplets)
        back.write("kg_data:")
        back.write(kg_data)

        # è®¡ç®—æ¦‚ç‡ - ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‚æ•°ç±»å‹
        if status_callback:
            status_callback("Calculating event probabilities based on context")
        prob_input = json.dumps({
            "event_description": query,
            "context": kg_data
        })
        probabilities = self.prob_tool.run(prob_input)
        back.write("probabilities:")
        back.write(probabilities)
        
        # Step 7: åŸºäºå®Œæ•´ä¿¡æ¯è¿›è¡Œæœ€ç»ˆåˆ†æ
        if status_callback:
            status_callback("ğŸ“ˆ Final analysis")
        impact_analysis = self._analyze_impact_with_context(query, updated_timeline, kg_data, probabilities)
        back.write("impact_analysis:")
        back.write(impact_analysis)  

        return {
            "timeline": updated_timeline,
            "knowledge_graph": kg_data,
            "triplets": triplets,
            "impact_analysis": impact_analysis,
            "search_results": all_results
        }
    
    def _generate_timeline_search_terms(self, query: str) -> List[str]:
        """Generate search keywords for building timeline framework"""
        prompt = f"""
        You are a professional search query optimization expert. Please generate 3-5 most effective Tavily search keywords for the following query:

        Original query: {query}

        Optimization requirements:
        1. Translate original query to English and use it for searching
        2. Include specific time ranges or dates (e.g., "2024", "last 3 months")
        3. Use precise event names and key entities
        4. Include timeline-related keywords ("timeline", "chronology", "sequence of events")
        5. Use English keywords for better search results
        6. Avoid overly broad queries, maintain specificity

        Examples:
        - For "Apple company development history" â†’ ["Apple Inc historical timeline 2020-2024", "Apple major events chronology", "Timeline of Apple product releases"]
        - For "COVID-19 pandemic development" â†’ ["COVID-19 pandemic timeline 2020-2024", "Coronavirus key events chronology", "Major COVID-19 milestones timeline"]

        Return keywords in JSON array format in English.
        """
        
        response = self.llm.invoke(prompt)
        try:
            return json.loads(response.content)
        except:
            return [f"{query} timeline 2020-2024", f"{query} key events chronology", f"{query} major milestones"]

    def _generate_supplemental_search_terms(self, query: str, timeline: List[Dict]) -> List[str]:
        """åŸºäºæ—¶é—´çº¿ä¿¡æ¯ç”Ÿæˆè¡¥å……æœç´¢å…³é”®è¯"""
        # ä»æ—¶é—´çº¿ä¸­æå–å…³é”®å®ä½“å’Œäº‹ä»¶
        timeline_text = "\n".join([f"{item.get('date', '')}: {item.get('title', '')}" for item in timeline[:5]])
        
        prompt = f"""
        You are a professional search query optimization expert. Based on the following timeline information and original query, generate 3-5 most effective Tavily search keywords:

        Original query: {query}
        Preliminary timeline:
        {timeline_text}

        Optimization requirements:
        1. Translate original query to English and use it for searching
        2. Generate precise search queries for specific events and entities in the timeline
        3. Include keywords related to causal relationships, impact analysis, and detailed insights
        4. Use English keywords for better search results
        5. Include specific time ranges or event names
        6. Avoid duplicate timeline searches, focus on supplementary details

        Examples:
        - For "iPhone release" event in timeline â†’ ["iPhone market impact analysis", "Apple iPhone sales statistics", "iPhone technological innovations"]
        - For "pandemic outbreak" event in timeline â†’ ["COVID-19 economic impact studies", "Pandemic healthcare system effects", "Coronavirus vaccine efficacy data"]

        Return keywords in JSON array format in English.
        """
        
        response = self.llm.invoke(prompt)
        try:
            return json.loads(response.content)
        except:
            # åŸºäºæ—¶é—´çº¿å†…å®¹ç”Ÿæˆæ›´å…·ä½“çš„åå¤‡å…³é”®è¯
            timeline_keywords = []
            for item in timeline[:3]:
                title = item.get('title', '')
                if title:
                    timeline_keywords.extend([f"{title} impact analysis", f"{title} detailed report", f"{title} statistics"])
            
            return timeline_keywords if timeline_keywords else [f"{query} impact analysis", f"{query} detailed report", f"{query} statistics"]

    def _create_detailed_timeline(self, search_results: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨AIç›´æ¥æ•´ç†æœç´¢ç»“æœæˆTimeline Tableæ ¼å¼ï¼ˆTime, Description, SourceLinkï¼‰"""
        try:

            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œæ•´ç†æˆç»“æ„åŒ–çš„æ—¶é—´çº¿è¡¨æ ¼(Timeline Table)ã€‚è¦æ±‚ï¼š

            è¾“å…¥æ•°æ®ï¼š{json.dumps(search_results, ensure_ascii=False, indent=2)}
            
            è¾“å‡ºè¦æ±‚ï¼š
            1. ä»å†…å®¹ä¸­æå–æˆ–æ¨æ–­æ—¶é—´ä¿¡æ¯ï¼ˆTimeï¼‰ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            2. æå–å…³é”®äº‹ä»¶æè¿°ï¼ˆDescriptionï¼‰ï¼Œç®€æ´æ˜äº†
            3. ä¿ç•™æ¥æºé“¾æ¥ï¼ˆSourceLinkï¼‰
            4. æŒ‰æ—¶é—´é¡ºåºæ’åº
            5. å¦‚æœæ— æ³•ç¡®å®šå…·ä½“æ—¥æœŸï¼Œå¯ä»¥ä½¿ç”¨æœˆä»½æˆ–å¹´ä»½
            6. ç¡®ä¿æ¯ä¸ªæ¡ç›®åŒ…å«Timeã€Descriptionã€SourceLinkä¸‰ä¸ªå­—æ®µ
            
            è¯·è¿”å›JSONæ ¼å¼çš„æ—¶é—´çº¿æ•°ç»„ï¼Œæ¯ä¸ªæ¡ç›®æ ¼å¼ï¼š
            {{
                "Time": "2024-01-15",
                "Description": "äº‹ä»¶æè¿°",
                "SourceLink": "https://example.com"
            }}
            
            åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
            """
            
            # ä½¿ç”¨AIæ¨¡å‹å¤„ç†
            response = self.llm.invoke(prompt)
            
            # è§£æAIè¿”å›çš„JSON
            timeline = json.loads(response.content.strip())
            
            
            return timeline
            
        except Exception as e:

            return self._create_fallback_timeline(search_results)
    
    def _create_fallback_timeline(self, search_results: List[Dict]) -> List[Dict]:
        """å¤‡ç”¨æ–¹æ³•ï¼šåŸå§‹çš„æ—¶é—´çº¿åˆ›å»ºé€»è¾‘"""
        timeline = []
        
        for result in search_results:
            if isinstance(result, dict):
                # å°è¯•ä»å„ç§å­—æ®µä¸­æå–æ—¶é—´ä¿¡æ¯
                time_field = None
                for field in ['published_date', 'date', 'timestamp', 'created_at']:
                    if field in result and result[field]:
                        time_field = result[field]
                        break
                
                if time_field:
                    try:
                        # ç®€åŒ–æ—¶é—´å¤„ç†
                        time_str = str(time_field)[:10]  # å–å‰10ä¸ªå­—ç¬¦
                        timeline.append({
                            "Time": time_str,
                            "Description": result.get('title', result.get('content', '')[:100]),
                            "SourceLink": result.get('url', '')
                        })
                    except:
                        continue
        
        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x.get('Time', ''))
        return timeline

    def _update_timeline_with_supplemental(self, timeline: List[Dict[str, Any]], supplemental_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨AIç»“åˆå·²æœ‰æ—¶é—´çº¿å’Œè¡¥å……ä¿¡æ¯æ¥å®Œå–„Timeline table"""
        if not supplemental_results:
            #st.write("âš ï¸ è¡¥å……æœç´¢ç»“æœä¸ºç©ºï¼Œæ— éœ€æ›´æ–°æ—¶é—´çº¿")
            return timeline
        
        #st.write(f"ğŸ“Š å¼€å§‹ä½¿ç”¨AIå®Œå–„æ—¶é—´çº¿ï¼Œå·²æœ‰ {len(timeline)} æ¡è®°å½•ï¼Œæ–°å¢ {len(supplemental_results)} æ¡è¡¥å……ç»“æœ")
        
        try:
            # æ„å»ºAIæç¤ºè¯ï¼Œè®©AIç»“åˆå·²æœ‰æ—¶é—´çº¿å’Œè¡¥å……ä¿¡æ¯æ¥å®Œå–„æ—¶é—´çº¿
            prompt = f"""
            è¯·ç»“åˆä»¥ä¸‹å·²æœ‰çš„æ—¶é—´çº¿æ•°æ®å’Œæ–°çš„è¡¥å……æœç´¢ç»“æœï¼Œå®Œå–„å’Œæ›´æ–°æ—¶é—´çº¿è¡¨æ ¼ï¼š

            å·²æœ‰æ—¶é—´çº¿æ•°æ®ï¼š
            {json.dumps(timeline, ensure_ascii=False, indent=2)}

            æ–°çš„è¡¥å……æœç´¢ç»“æœï¼š
            {json.dumps(supplemental_results, ensure_ascii=False, indent=2)}

            å¤„ç†è¦æ±‚ï¼š
            1. æ•´åˆå·²æœ‰æ—¶é—´çº¿å’Œæ–°çš„è¡¥å……ä¿¡æ¯
            2. å»é‡ç›¸åŒæ—¶é—´ç‚¹çš„æ¡ç›®
            3. è¡¥å……ç¼ºå¤±çš„æ—¶é—´ä¿¡æ¯ï¼ˆä»å†…å®¹ä¸­æ¨æ–­ï¼‰
            4. å®Œå–„äº‹ä»¶æè¿°çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
            5. ç¡®ä¿æ¯ä¸ªæ¡ç›®åŒ…å«Timeã€Descriptionã€SourceLinkä¸‰ä¸ªå­—æ®µ
            6. æŒ‰æ—¶é—´é¡ºåºæ’åº

            è¾“å‡ºæ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªæ¡ç›®æ ¼å¼ï¼š
            {{
                "Time": "2024-01-15",
                "Description": "å®Œæ•´çš„äº‹ä»¶æè¿°",
                "SourceLink": "https://example.com"
            }}

            è¯·è¿”å›å®Œå–„åçš„æ—¶é—´çº¿JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
            """
            
            # ä½¿ç”¨AIæ¨¡å‹å¤„ç†
            response = self.llm.invoke(prompt)
            
            # è§£æAIè¿”å›çš„JSON
            updated_timeline = json.loads(response.content.strip())
            

            return updated_timeline
            
        except Exception as e:

            return self._update_fallback_timeline(timeline, supplemental_results)
    
    def _update_fallback_timeline(self, timeline: List[Dict[str, Any]], supplemental_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤‡ç”¨æ–¹æ³•ï¼šåŸå§‹çš„æ—¶é—´çº¿æ›´æ–°é€»è¾‘"""
        new_timeline = timeline.copy()
        
        for result in supplemental_results:
            if isinstance(result, dict):
                # å°è¯•ä»å„ç§å­—æ®µä¸­æå–æ—¶é—´ä¿¡æ¯
                time_field = None
                for field in ['published_date', 'date', 'timestamp', 'created_at']:
                    if field in result and result[field]:
                        time_field = result[field]
                        break
                
                if time_field:
                    try:
                        time_str = str(time_field)[:10]
                        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥æ—¶é—´çš„æ¡ç›®
                        existing_entry = next((item for item in new_timeline if item.get('Time') == time_str), None)
                        
                        if existing_entry:
                            # æ›´æ–°ç°æœ‰æ¡ç›®æè¿°
                            existing_desc = existing_entry.get('Description', '')
                            new_info = result.get('title', result.get('content', '')[:100])
                            if new_info not in existing_desc:
                                existing_entry['Description'] = f"{existing_desc} | {new_info}"
                        else:
                            # æ·»åŠ æ–°æ¡ç›®
                            new_timeline.append({
                                "Time": time_str,
                                "Description": result.get('title', result.get('content', '')[:100]),
                                "SourceLink": result.get('url', '')
                            })
                    except:
                        continue
        
        # æŒ‰æ—¶é—´æ’åº
        new_timeline.sort(key=lambda x: x.get('Time', ''))
        return new_timeline

    def _generate_ai_structured_timeline(self, timeline: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨AIåˆ†æç”Ÿæˆç»“æ„åŒ–çš„æ—¶é—´çº¿è¡¨æ ¼"""
        if not timeline:
            return timeline
            
        prompt = f"""
        You are a professional historical event analyst. Based on the following timeline information and original query, generate a structured timeline table:

        Original query: {query}
        Search results timeline:
        {timeline}

        Please analyze this information and generate a clearer, more organized timeline with the following fields:
        1. Date (format: YYYY-MM-DD)
        2. Event title (concise and clear)
        3. Event description (detailed explanation)
        4. Source link (if available)
        5. Importance score (1-10 points)

        Return in JSON array format, each object containing: date, title, description, source, importance_score

        Note: Maintain event authenticity and accuracy, do not add fictional information.
        """
        
        try:
            response = self.llm.invoke(prompt)
            structured_timeline = json.loads(response.content)
            
            # åˆå¹¶åŸå§‹æ¥æºä¿¡æ¯
            for item in structured_timeline:
                original_item = next((t for t in timeline if t['date'] == item.get('date', '')), None)
                if original_item and 'source' in original_item:
                    item['source'] = original_item['source']
            
            return structured_timeline
        except:
            # å¦‚æœAIåˆ†æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ—¶é—´çº¿
            return timeline

    def _extract_triplets_from_all_sources(self, search_results: List[Dict[str, Any]], query: str) -> List[Tuple[str, str, str]]:
        """ä»æ‰€æœ‰æœç´¢ç»“æœä¸­æå–çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„"""
        all_text = ' '.join([str(result.get('content', '')) for result in search_results if isinstance(result, dict)])
        
        # ä½¿ç”¨æ–°çš„AIçŸ¥è¯†å›¾è°±è½¬æ¢å™¨
        return self.converter.text_to_triplets(all_text, query)

    def _create_knowledge_graph_from_triplets(self, triplets: List[Tuple[str, str, str]]) -> Dict[str, Any]:
        """ä»ä¸‰å…ƒç»„åˆ›å»ºçŸ¥è¯†å›¾è°±"""
        kg_graph = self.converter.create_knowledge_graph(triplets)
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        kg_data = {
            'nodes': [],
            'edges': []
        }
        
        for node in kg_graph.nodes():
            kg_data['nodes'].append({
                'id': node,
                'label': node,
                'type': kg_graph.nodes[node].get('type', 'entity')
            })
        
        for edge in kg_graph.edges(data=True):
            kg_data['edges'].append({
                'from': edge[0],
                'to': edge[1],
                'relation': edge[2].get('relation', ''),
                'label': edge[2].get('label', '')
            })
        
        return kg_data

    def _analyze_impact_with_context(self, query: str, timeline: List[Dict[str, Any]], kg_data: Dict[str, Any], probabilities: Dict[str, float]) -> str:
        """åŸºäºæ—¶é—´çº¿å’ŒçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡è¿›è¡Œå½±å“åˆ†æ"""

        
        # å‡†å¤‡ä¸‰å…ƒç»„ä¿¡æ¯
        triplets_info = ""
        if 'edges' in kg_data:
            for edge in kg_data['edges'][:30]:  # å¢åŠ æ•°é‡ä»¥æ”¯æŒæ›´å¤šåœºæ™¯
                triplets_info += f"({edge['from']}, {edge['relation']}, {edge['to']})\n"
        
        prompt = f"""
        Based on the following complete information for analysis:

        Original query: {query}
        Probability analysis results: {json.dumps(probabilities, ensure_ascii=False)}
        Knowledge graph information: {json.dumps(kg_data, ensure_ascii=False)}
        Timeline information:
        {timeline}
        
        Knowledge graph triplets:
        {triplets_info}
        
        Please provide detailed impact prediction analysis in the following format:

        ### Prediction Analysis Results
        #### Comprehensive Assessment Based on Timeline and Knowledge Graph

        ### Short-term Predictions (Next 1-6 months)
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing timeline events and triplets) |
        |-------------|-------------|---------------------|-------------|--------------------------------------------|
        | 1 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 2 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 3 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 4 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 5 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 6 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 7 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |
        | 8 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite specific timeline events and related triplets] |

        ### Medium-term Predictions (Next 6 months - 2 years)
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing timeline trends and triplet relationships) |
        |-------------|-------------|---------------------|-------------|---------------------------------------------------------|
        | 1 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 2 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 3 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 4 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 5 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 6 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 7 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |
        | 8 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite timeline trends and related triplet relationships] |

        ### Long-term Predictions (Beyond 2 years)
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing structural changes and triplet patterns) |
        |-------------|-------------|---------------------|-------------|-------------------------------------------------------|
        | 1 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 2 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 3 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 4 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 5 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 6 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 7 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |
        | 8 | [Specific scenario description] | [Trigger event/factor] | [Probability]% | [Cite structural changes and triplet patterns] |

        Requirements:
        1. Provide 5-8 main scenarios for each time period (minimum 5, maximum 8)
        2. Probability should be calculated based on timeline event frequency and knowledge graph relationship strength
        3. Clearly cite specific timeline events and knowledge graph triplets as basis
        4. Total probability for each time period should be 100%
        5. Respond in English
        6. Ensure scenario descriptions are specific, detailed, and have practical basis
        """
        
        response = self.llm.invoke(prompt)
        return response.content

    def _extract_entities(self, query: str) -> List[str]:
        """Extract entities from query"""
        prompt = f"""
        Extract key entity names from the following query:
        Query: {query}
        
        Return entity names in JSON array format, e.g.: ["entity1", "entity2", "entity3"]
        """
        
        response = self.llm.invoke(prompt)
        return json.loads(response.content)

    def _generate_report(self, query: str, probabilities: Dict[str, float], kg_info: Dict) -> str:
        """Generate probability analysis report"""
        prompt = f"""
        Based on the following information, generate a detailed analysis report:
        
        Original query: {query}
        Probability analysis results: {json.dumps(probabilities, ensure_ascii=False)}
        Knowledge graph information: {json.dumps(kg_info, ensure_ascii=False)}
        
        Please provide a detailed probability analysis report in the following format:

        ### Short-term (next 1-6 months)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        ### Medium-term (6 months - 2 years)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        ### Long-term (2+ years)
        #### Step 1 & 2: Possible scenarios and probabilities
        | Scenario ID | Description | Key Trigger Factors | Probability | Basis (citing triplets) |
        |----------|------|--------------|------|-------------------|
        | 1 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 2 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 3 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 4 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 5 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 6 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 7 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |
        | 8 | [Scenario description] | [Trigger factors] | [Probability]% | [Cite triplets and explanation] |

        Requirements:
        1. Provide 5-8 main scenarios for each time period (minimum 5, maximum 8)
        2. Total probability for each time period should be 100%
        3. Clearly cite knowledge graph triplets as basis
        4. Scenario descriptions should be specific, detailed, and have practical basis
        5. Respond in English
        """
        
        response = self.llm.invoke(prompt)
        return response.content


class MultiAgentWorkflowSystem:
    """å¤šAgentå·¥ä½œæµç³»ç»Ÿ"""
    
    def __init__(self, api_key: str, api_version: str, azure_endpoint: str, cue: str ,use_deepseek: bool = True):
        # åˆå§‹åŒ–LLM - æ”¯æŒDeepSeekå’ŒAzure OpenAI

        # ä½¿ç”¨DeepSeek API
        self.llm: ChatOpenAI = ChatOpenAI(
            api_key=api_key,  # ä½¿ç”¨DeepSeek API key
            base_url="https://api.deepseek.com/v1",
            model="deepseek-chat"
        )
        # åˆå§‹åŒ–å·¥å…·
        self.search_tool = TavilySearchTool()
        self.kg_tool = KnowledgeGraphTool()
        self.prob_tool = ProbabilityCalculatorTool()
        self.cue = cue
        # åˆå§‹åŒ–Agent
        self.supervisor = SupervisorAgent(self.llm)
        self.workflow1 = Workflow1Agent(self.llm, self.search_tool, self.kg_tool,self.prob_tool)
        #self.workflow2 = Workflow2Agent(self.llm, self.prob_tool, self.kg_tool)
    
    def _format_timeline_summary(self, timeline_data: List[Dict]) -> str:
        """Format timeline information summary"""
        if not timeline_data:
            return ""
        
        summary = "ğŸ“… **Timeline Information Summary:**\n\n"
        
        # Group by date
        timeline_by_date = {}
        for item in timeline_data:
            date = item.get('date', 'Unknown date')
            if date not in timeline_by_date:
                timeline_by_date[date] = []
            timeline_by_date[date].append(item)
        
        # Generate summary
        for date, events in timeline_by_date.items():
            summary += f"**{date}**:\n"
            for event in events[:3]:  # Maximum 3 events per date
                title = event.get('title', 'No title')
                content_preview = event.get('content', '')[:50] + '...' if len(event.get('content', '')) > 50 else event.get('content', '')
                summary += f"  â€¢ {title}: {content_preview}\n"
            if len(events) > 3:
                summary += f"  There are {len(events) - 3} more related events...\n"
            summary += "\n"
        
        return summary

    def _format_probability_summary(self, probabilities: Dict[str, float]) -> str:
        """Format probability information summary"""
        if not probabilities:
            return ""
        
        summary = "ğŸ“Š **Probability Analysis Summary:**\n\n"
        
        # Sort by probability
        sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
        
        for scenario, prob in sorted_probs:
            percentage = prob * 100
            summary += f"â€¢ {scenario}: {percentage:.1f}%\n"
        
        return summary

    def process_query(self, query: str, status_callback=None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        # åˆ†ææ„å›¾
        if status_callback:
            status_callback("Analysing user intention to determine workflow")
        intent = self.supervisor.analyze_intent(query)
        st.session_state.intent = intent

        # æ ¹æ®æ„å›¾è·¯ç”±åˆ°å¯¹åº”å·¥ä½œæµ
        if intent == 'workflow1':
            if status_callback:
                status_callback("Processing with Workflow1: Event impact analysis")
            raw_result = self.workflow1.process(query, status_callback)
            
            # æ ¼å¼åŒ–å“åº”ï¼Œæ•´åˆæ—¶é—´çº¿ä¿¡æ¯ï¼Œä¸è¾“å‡ºåŸå§‹æœç´¢ç»“æœå’ŒçŸ¥è¯†å›¾è°±
            timeline_summary = self._format_timeline_summary(raw_result.get('timeline', []))
            impact_analysis = raw_result.get('impact_analysis', '')
            
            formatted_response = f"{impact_analysis}\n\n{timeline_summary}"
            
            result = {
                'workflow_type': 'workflow1',
                'response': formatted_response,
                'timeline': raw_result.get('timeline', []),
                'knowledge_graph': raw_result.get('knowledge_graph', {}),
                'search_results': raw_result.get('search_results', [])
            }

        else:
            # æ­£å¸¸èŠå¤©å¤„ç†
            if status_callback:
                status_callback("Processing with normal chat mode")

            prompt_template = ChatPromptTemplate.from_messages([
                                ("system", self.cue),
                                ("human", "{query}")
                            ])
            prompt = prompt_template.format(query=query)
            response = self.llm.invoke(prompt)
            
            result = {
                'workflow_type': 'normal',
                'response': response.content
            }
        
        return result

# Streamlitå¯è§†åŒ–å·¥å…·
